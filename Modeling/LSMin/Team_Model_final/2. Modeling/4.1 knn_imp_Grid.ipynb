{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 모듈 및 데이터 로딩\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "file = '../0. Data/2. output/3. knn_imp_Train_data.csv'\n",
    "data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# KNN Imputer 적용\u001b[39;00m\n\u001b[0;32m     10\u001b[0m imputer \u001b[38;5;241m=\u001b[39m KNNImputer(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m data_imputed[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_pv\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_imputed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscale_pv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 예측된 scale_pv 값을 2초과 4미만 범위로 조정\u001b[39;00m\n\u001b[0;32m     15\u001b[0m data_imputed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_pv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(data_imputed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_pv\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m2.01\u001b[39m, \u001b[38;5;241m3.99\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\impute\\_knn.py:357\u001b[0m, in \u001b[0;36mKNNImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# process in fixed-memory chunks\u001b[39;00m\n\u001b[0;32m    349\u001b[0m gen \u001b[38;5;241m=\u001b[39m pairwise_distances_chunked(\n\u001b[0;32m    350\u001b[0m     X[row_missing_idx, :],\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    355\u001b[0m     reduce_func\u001b[38;5;241m=\u001b[39mprocess_chunk,\n\u001b[0;32m    356\u001b[0m )\n\u001b[1;32m--> 357\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m gen:\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# process_chunk modifies X in place. No return value.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_empty_features:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2017\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2016\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[1;32m-> 2017\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   2019\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   2021\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   2022\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   2023\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:2195\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2192\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m   2193\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 2195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1765\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1762\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:512\u001b[0m, in \u001b[0;36mnan_euclidean_distances\u001b[1;34m(X, Y, squared, missing_values, copy)\u001b[0m\n\u001b[0;32m    510\u001b[0m present_Y \u001b[38;5;241m=\u001b[39m present_X \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m X \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m~\u001b[39mmissing_Y\n\u001b[0;32m    511\u001b[0m present_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(present_X, present_Y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m--> 512\u001b[0m distances[present_count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# avoid divide by zero\u001b[39;00m\n\u001b[0;32m    514\u001b[0m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;241m1\u001b[39m, present_count, out\u001b[38;5;241m=\u001b[39mpresent_count)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### 모델 생성을 위한 Train 데이터 증강 by KNN Imputer\n",
    "\n",
    "\n",
    "# scale_pv가 2 이하인 데이터를 NaN으로 대체\n",
    "data_imputed = data.copy()\n",
    "data_imputed.loc[data_imputed['scale_pv'] <= 2, 'scale_pv'] = np.nan\n",
    "\n",
    "\n",
    "# KNN Imputer 적용\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "data_imputed[['scale_pv']] = imputer.fit_transform(data_imputed[['scale_pv']])\n",
    "\n",
    "\n",
    "# 예측된 scale_pv 값을 2초과 4미만 범위로 조정\n",
    "data_imputed['scale_pv'] = np.clip(data_imputed['scale_pv'], 2.01, 3.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Linear Regression: {'copy_X': True, 'fit_intercept': False, 'n_jobs': None, 'positive': True}\n",
      "Best CV score (MAE) for Linear Regression: 0.37341008169551293\n",
      "Best parameters for Random Forest: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best CV score (MAE) for Random Forest: 0.37507885667090607\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005243 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 107248, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Best parameters for LightGBM: {'learning_rate': 0.01, 'max_depth': -1, 'n_estimators': 50, 'num_leaves': 31}\n",
      "Best CV score (MAE) for LightGBM: 0.353598953591645\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 227\n",
      "[LightGBM] [Info] Number of data points in the train set: 107248, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "Linear Regression - Train MAE: 0.0073317642236211495, Train MAPE: 0.24131408996224535\n",
      "Linear Regression - Valid MAE: 0.007579622844245958, Valid MAPE: 0.2494129051709395\n",
      "\n",
      "Random Forest - Train MAE: 0.007161773702566194, Train MAPE: 0.2355909295103621\n",
      "Random Forest - Valid MAE: 0.0075673345406404025, Valid MAPE: 0.24875298222709488\n",
      "\n",
      "LightGBM - Train MAE: 0.006926887773645037, Train MAPE: 0.22795572809497844\n",
      "LightGBM - Valid MAE: 0.007162495333905666, Valid MAPE: 0.23563492919337492\n"
     ]
    }
   ],
   "source": [
    "### 모델링 및 Train 평가\n",
    "\n",
    "\n",
    "# 중복값 제거\n",
    "data_imputed.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X = data_imputed.drop(columns=['scale_pv'])\n",
    "y = data_imputed['scale_pv']\n",
    "\n",
    "\n",
    "# 학습 데이터와 검증 데이터 분리\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "\n",
    "# 타겟 스케일링\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_valid_scaled = target_scaler.transform(y_valid.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "### Grid Search를 사용한 최적의 파라미터 찾기\n",
    "\n",
    "# Linear Regression - GridSearchCV\n",
    "lr_param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'copy_X': [True, False],\n",
    "    'n_jobs': [None, -1],\n",
    "    'positive': [True, False]\n",
    "}\n",
    "lr_grid_search = GridSearchCV(LinearRegression(), lr_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "lr_grid_search.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "print(f\"Best parameters for Linear Regression: {lr_grid_search.best_params_}\")\n",
    "print(f\"Best CV score (MAE) for Linear Regression: {-lr_grid_search.best_score_}\")\n",
    "\n",
    "# Random Forest - GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "rf_grid_search = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "rf_grid_search.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "print(f\"Best parameters for Random Forest: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best CV score (MAE) for Random Forest: {-rf_grid_search.best_score_}\")\n",
    "\n",
    "# LightGBM - GridSearchCV\n",
    "lgb_param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [-1, 10, 20, 30]\n",
    "}\n",
    "lgb_grid_search = GridSearchCV(lgb.LGBMRegressor(random_state=42), lgb_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "lgb_grid_search.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "print(f\"Best parameters for LightGBM: {lgb_grid_search.best_params_}\")\n",
    "print(f\"Best CV score (MAE) for LightGBM: {-lgb_grid_search.best_score_}\")\n",
    "\n",
    "\n",
    "# 최적의 모델로 학습 및 평가\n",
    "\n",
    "# 최적의 파라미터로 모델 재학습\n",
    "lr_best_model = lr_grid_search.best_estimator_\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "lgb_best_model = lgb_grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가 함수\n",
    "def train_and_evaluate_model(model, X_train, X_valid, y_train_scaled, y_valid_scaled, target_scaler):\n",
    "    model.fit(X_train, y_train_scaled.ravel())\n",
    "    y_train_pred_scaled = model.predict(X_train)\n",
    "    y_valid_pred_scaled = model.predict(X_valid)\n",
    "    \n",
    "    # 역스케일링\n",
    "    y_train_pred = target_scaler.inverse_transform(y_train_pred_scaled.reshape(-1, 1))\n",
    "    y_valid_pred = target_scaler.inverse_transform(y_valid_pred_scaled.reshape(-1, 1))\n",
    "    y_train_original = target_scaler.inverse_transform(y_train_scaled)\n",
    "    y_valid_original = target_scaler.inverse_transform(y_valid_scaled)\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train_original, y_train_pred)\n",
    "    valid_mae = mean_absolute_error(y_valid_original, y_valid_pred)\n",
    "    train_mape = mean_absolute_percentage_error(y_train_original, y_train_pred)\n",
    "    valid_mape = mean_absolute_percentage_error(y_valid_original, y_valid_pred)\n",
    "    \n",
    "    return train_mae, valid_mae, train_mape, valid_mape, y_train_pred, y_valid_pred\n",
    "\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "\n",
    "# Multiple Regression\n",
    "lr_train_mae, lr_valid_mae, lr_train_mape, lr_valid_mape, lr_y_train_pred, lr_y_valid_pred = train_and_evaluate_model(lr_best_model, X_train_scaled, X_valid_scaled, y_train_scaled, y_valid_scaled, target_scaler)\n",
    "\n",
    "# Random Forest\n",
    "rf_train_mae, rf_valid_mae, rf_train_mape, rf_valid_mape, rf_y_train_pred, rf_y_valid_pred = train_and_evaluate_model(rf_best_model, X_train_scaled, X_valid_scaled, y_train_scaled, y_valid_scaled, target_scaler)\n",
    "\n",
    "# LightGBM\n",
    "lgb_train_mae, lgb_valid_mae, lgb_train_mape, lgb_valid_mape, lgb_y_train_pred, lgb_y_valid_pred = train_and_evaluate_model(lgb_best_model, X_train_scaled, X_valid_scaled, y_train_scaled, y_valid_scaled, target_scaler)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Linear Regression - Train MAE: {lr_train_mae}, Train MAPE: {lr_train_mape*100}\")\n",
    "print(f\"Linear Regression - Valid MAE: {lr_valid_mae}, Valid MAPE: {lr_valid_mape*100}\")\n",
    "print()\n",
    "print(f\"Random Forest - Train MAE: {rf_train_mae}, Train MAPE: {rf_train_mape*100}\")\n",
    "print(f\"Random Forest - Valid MAE: {rf_valid_mae}, Valid MAPE: {rf_valid_mape*100}\")\n",
    "print()\n",
    "print(f\"LightGBM - Train MAE: {lgb_train_mae}, Train MAPE: {lgb_train_mape*100}\")\n",
    "print(f\"LightGBM - Valid MAE: {lgb_valid_mae}, Valid MAPE: {lgb_valid_mape*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# import joblib\n",
    "\n",
    "# # 모델 및 스케일러 저장\n",
    "# scaler_data = {\n",
    "#     'scaler': scaler,\n",
    "#     'feature_names': X.columns.tolist(),\n",
    "#     'target_scaler': target_scaler\n",
    "# }\n",
    "# joblib.dump(rf_best_model, '../2. Modeling/model/kIG_rf_model.pkl')\n",
    "# joblib.dump(scaler_data, '../2. Modeling/model/kIG_rf_scaler.pkl')\n",
    "\n",
    "# print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test - Linear Regression MAE: 0.023359419107954284, MAPE: 0.7632538471508572\n",
      "Final Test - Random Forest MAE: 0.023537679168576266, MAPE: 0.7693245496258292\n",
      "Final Test - LightGBM MAE: 0.023668570021191427, MAPE: 0.7732892630046533\n"
     ]
    }
   ],
   "source": [
    "### Test 평가\n",
    "\n",
    "# Test_data 로딩\n",
    "test_file = '../0. Data/2. output/0. Test_data.csv'\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# 피처와 타겟 분리\n",
    "X_test_final = test_data.drop(columns=['scale_pv'])\n",
    "y_test_final = test_data['scale_pv']\n",
    "\n",
    "# 테스트 데이터 스케일링\n",
    "X_test_final_scaled = scaler.transform(X_test_final)\n",
    "y_test_final_scaled = target_scaler.transform(y_test_final.values.reshape(-1, 1))\n",
    "\n",
    "# 최종 테스트 데이터 예측 및 평가\n",
    "def final_evaluate_model(model, X_test_scaled, y_test_scaled, target_scaler):\n",
    "    y_test_pred_scaled = model.predict(X_test_scaled)\n",
    "    \n",
    "    # 역스케일링\n",
    "    y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled.reshape(-1, 1))\n",
    "    y_test_original = target_scaler.inverse_transform(y_test_scaled)\n",
    "    \n",
    "    test_mae = mean_absolute_error(y_test_original, y_test_pred)\n",
    "    test_mape = mean_absolute_percentage_error(y_test_original, y_test_pred)\n",
    "    \n",
    "    return test_mae, test_mape, y_test_pred\n",
    "\n",
    "# 최종 평가 결과\n",
    "lr_test_mae_final, lr_test_mape_final, lr_y_test_pred_final = final_evaluate_model(lr_best_model, X_test_final_scaled, y_test_final_scaled, target_scaler)\n",
    "rf_test_mae_final, rf_test_mape_final, rf_y_test_pred_final = final_evaluate_model(rf_best_model, X_test_final_scaled, y_test_final_scaled, target_scaler)\n",
    "lgb_test_mae_final, lgb_test_mape_final, lgb_y_test_pred_final = final_evaluate_model(lgb_best_model, X_test_final_scaled, y_test_final_scaled, target_scaler)\n",
    "\n",
    "print(f\"Final Test - Linear Regression MAE: {lr_test_mae_final}, MAPE: {lr_test_mape_final*100}\")\n",
    "print(f\"Final Test - Random Forest MAE: {rf_test_mae_final}, MAPE: {rf_test_mape_final*100}\")\n",
    "print(f\"Final Test - LightGBM MAE: {lgb_test_mae_final}, MAPE: {lgb_test_mape_final*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
