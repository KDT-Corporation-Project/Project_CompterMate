{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline으로 전처리 - 모델링 - 예측까지 한번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikitlearn update\n",
    "# %conda install -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_scr_pv</th>\n",
       "      <th>c_temp_pv</th>\n",
       "      <th>k_rpm_pv</th>\n",
       "      <th>n_temp_pv</th>\n",
       "      <th>scale_pv</th>\n",
       "      <th>s_temp_pv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.9</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   E_scr_pv  c_temp_pv  k_rpm_pv  n_temp_pv  scale_pv  s_temp_pv\n",
       "0         8       69.6       189       67.2      3.01       67.1\n",
       "1         8       69.8       189       67.2      3.01       67.0\n",
       "2         8       69.7       189       67.9      3.08       65.9\n",
       "3         8       69.7       189       67.8      3.08       65.9\n",
       "4         8       69.7       189       67.8      3.08       65.9"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../DATA/바웰공정데이터.csv')\n",
    "\n",
    "# 2. Preprocessing : 목요일까지의 전처리\n",
    "# (1) 2 < scale_pv < 4\n",
    "data = data[(data['scale_pv'] > 2) & (data['scale_pv'] < 4)]\n",
    "\n",
    "# (2) k_rpm_pv 가 100 이하인 행 제거\n",
    "data = data[data['k_rpm_pv'] > 100]\n",
    "\n",
    "# (3) n_temp_sv=0 인 행 제거\n",
    "data = data[data['n_temp_sv'] != 0]\n",
    "\n",
    "# (4) 컬럼 제거 : E_scr_sv, c_temp_sv, n_temp_sv, s_temp_sv, k_rpm_sv, time\n",
    "data = data.drop(['E_scr_sv', 'c_temp_sv', 'n_temp_sv', 's_temp_sv', \"k_rpm_sv\", 'time'], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2. Preprocessing : 추가 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2519\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 중복값 확인\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# 중복값 제거\n",
    "data = data.drop_duplicates()\n",
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 209\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "MinMaxScaler_LinearRegression - MAE : 0.0282\n",
      "MinMaxScaler_LinearRegression - MAPE : 0.009285\n",
      "MinMaxScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "StandardScaler_LinearRegression - MAE : 0.0282\n",
      "StandardScaler_LinearRegression - MAPE : 0.009285\n",
      "StandardScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "RobustScaler_LinearRegression - MAE : 0.0282\n",
      "RobustScaler_LinearRegression - MAPE : 0.009285\n",
      "RobustScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "MinMaxScaler_ElasticNet - MAE : 0.0288\n",
      "MinMaxScaler_ElasticNet - MAPE : 0.009469\n",
      "MinMaxScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "StandardScaler_ElasticNet - MAE : 0.0288\n",
      "StandardScaler_ElasticNet - MAPE : 0.009469\n",
      "StandardScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "RobustScaler_ElasticNet - MAE : 0.0288\n",
      "RobustScaler_ElasticNet - MAPE : 0.009469\n",
      "RobustScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "MinMaxScaler_RandomForestRegressor - MAE : 0.0225\n",
      "MinMaxScaler_RandomForestRegressor - MAPE : 0.007415\n",
      "MinMaxScaler_RandomForestRegressor - R2 : 0.4162\n",
      "----------------------------------\n",
      "StandardScaler_RandomForestRegressor - MAE : 0.0225\n",
      "StandardScaler_RandomForestRegressor - MAPE : 0.007397\n",
      "StandardScaler_RandomForestRegressor - R2 : 0.4265\n",
      "----------------------------------\n",
      "RobustScaler_RandomForestRegressor - MAE : 0.0226\n",
      "RobustScaler_RandomForestRegressor - MAPE : 0.007441\n",
      "RobustScaler_RandomForestRegressor - R2 : 0.4204\n",
      "----------------------------------\n",
      "MinMaxScaler_LGBMRegressor - MAE : 0.0256\n",
      "MinMaxScaler_LGBMRegressor - MAPE : 0.008420\n",
      "MinMaxScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "StandardScaler_LGBMRegressor - MAE : 0.0256\n",
      "StandardScaler_LGBMRegressor - MAPE : 0.008420\n",
      "StandardScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "RobustScaler_LGBMRegressor - MAE : 0.0256\n",
      "RobustScaler_LGBMRegressor - MAPE : 0.008420\n",
      "RobustScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature Engineering\n",
    "# - Pileline으로 스케일링 및 모델링을 한번에 처리\n",
    "# - Scaling : MinMaxScaler, StandardScaler, RobustScaler, 스케일링 없이 하나\n",
    "# - Model : LinearRegression, ElasticNet, RandomForest, LightGBM\n",
    "# - Evaluation : MAE, MAPE, R2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 3-1. 데이터 분할\n",
    "X = data.drop('scale_pv', axis=1)\n",
    "y = data['scale_pv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create pipelines for each scaling method and model\n",
    "pipelines = {\n",
    "    'MinMaxScaler_LinearRegression': Pipeline([('scaler', MinMaxScaler()), ('model', LinearRegression())]),\n",
    "    'StandardScaler_LinearRegression': Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),\n",
    "    'RobustScaler_LinearRegression': Pipeline([('scaler', RobustScaler()), ('model', LinearRegression())]),\n",
    "    'MinMaxScaler_ElasticNet': Pipeline([('scaler', MinMaxScaler()), ('model', ElasticNet())]),\n",
    "    'StandardScaler_ElasticNet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNet())]),\n",
    "    'RobustScaler_ElasticNet': Pipeline([('scaler', RobustScaler()), ('model', ElasticNet())]),\n",
    "    'MinMaxScaler_RandomForestRegressor': Pipeline([('scaler', MinMaxScaler()), ('model', RandomForestRegressor())]),\n",
    "    'StandardScaler_RandomForestRegressor': Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())]),\n",
    "    'RobustScaler_RandomForestRegressor': Pipeline([('scaler', RobustScaler()), ('model', RandomForestRegressor())]),\n",
    "    'MinMaxScaler_LGBMRegressor': Pipeline([('scaler', MinMaxScaler()), ('model', LGBMRegressor())]),\n",
    "    'StandardScaler_LGBMRegressor': Pipeline([('scaler', StandardScaler()), ('model', LGBMRegressor())]),\n",
    "    'RobustScaler_LGBMRegressor': Pipeline([('scaler', RobustScaler()), ('model', LGBMRegressor())]),\n",
    "}\n",
    "\n",
    "# Accessing a specific pipeline\n",
    "pipeline = pipelines['MinMaxScaler_LinearRegression']\n",
    "\n",
    "# Fit the pipeline\n",
    "for pipeline in pipelines.values():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate the pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f'{name} - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "    print(f'{name} - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "    print(f'{name} - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "NoScaler_LinearRegression - MAE : 0.0282\n",
      "NoScaler_LinearRegression - MAPE : 0.009285\n",
      "NoScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "NoScaler_ElasticNet - MAE : 0.0288\n",
      "NoScaler_ElasticNet - MAPE : 0.009469\n",
      "NoScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "NoScaler_RandomForestRegressor - MAE : 0.0225\n",
      "NoScaler_RandomForestRegressor - MAPE : 0.007421\n",
      "NoScaler_RandomForestRegressor - R2 : 0.4203\n",
      "----------------------------------\n",
      "NoScaler_LGBMRegressor - MAE : 0.0256\n",
      "NoScaler_LGBMRegressor - MAPE : 0.008420\n",
      "NoScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Scaling 안한 pipelines\n",
    "pipelines = {\n",
    "    'NoScaler_LinearRegression': Pipeline([('model', LinearRegression())]),\n",
    "    'NoScaler_ElasticNet': Pipeline([('model', ElasticNet())]),\n",
    "    'NoScaler_RandomForestRegressor': Pipeline([('model', RandomForestRegressor())]),\n",
    "    'NoScaler_LGBMRegressor': Pipeline([('model', LGBMRegressor())]),\n",
    "}\n",
    "\n",
    "# Fit the pipeline\n",
    "for pipeline in pipelines.values():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate the pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f'{name} - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "    print(f'{name} - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "    print(f'{name} - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "    print('----------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model - MAE : 0.0225\n",
      "Final Model - MAPE : 0.007392\n",
      "Final Model - R2 : 0.4266\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# StandardScaler, RandomForestRegressor로 최종 모델 선택\n",
    "final_pipeline = Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())])\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "print(f'Final Model - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'Final Model - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'Final Model - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wjs31\\OneDrive\\문서\\KDT5\\KDT5_NLP_Project\\.conda\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.6856 - val_loss: 0.0141\n",
      "Epoch 2/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0119 - val_loss: 0.0038\n",
      "Epoch 3/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0033 - val_loss: 0.0021\n",
      "Epoch 4/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 5/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 6/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 7/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 8/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 9/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 10/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 11/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 12/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 13/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 14/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 15/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 16/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 17/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 18/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 19/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 20/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 21/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 22/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 23/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 24/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 25/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 26/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 27/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 28/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 29/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 30/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 31/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 32/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 33/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 34/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 35/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 36/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 37/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 38/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 39/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 40/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 41/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "Epoch 42/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 43/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 44/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 45/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 46/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 47/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 48/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 49/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 50/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 51/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 52/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 53/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 54/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 55/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 56/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 57/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 58/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 59/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 60/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 61/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 62/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 63/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 64/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 65/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 66/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 67/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 68/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 69/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 70/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 71/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 72/1000\n",
      "\u001b[1m512/512\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 72: early stopping\n",
      "\u001b[1m160/160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "LSTM - MAE : 0.0283\n",
      "LSTM - MAPE : 0.009327\n",
      "LSTM - R2 : 0.0281\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델링\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# LSTM 모델링\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train_scaled.shape[1], 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)\n",
    "\n",
    "# Reshape\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "# Evaluate\n",
    "print(f'LSTM - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'LSTM - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'LSTM - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_MML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
