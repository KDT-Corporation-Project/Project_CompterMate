{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline으로 전처리 - 모델링 - 예측까지 한번에"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikitlearn update\n",
    "# %conda install -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_scr_pv</th>\n",
       "      <th>c_temp_pv</th>\n",
       "      <th>k_rpm_pv</th>\n",
       "      <th>n_temp_pv</th>\n",
       "      <th>scale_pv</th>\n",
       "      <th>s_temp_pv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>69.6</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>69.8</td>\n",
       "      <td>189</td>\n",
       "      <td>67.2</td>\n",
       "      <td>3.01</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.9</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>69.7</td>\n",
       "      <td>189</td>\n",
       "      <td>67.8</td>\n",
       "      <td>3.08</td>\n",
       "      <td>65.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   E_scr_pv  c_temp_pv  k_rpm_pv  n_temp_pv  scale_pv  s_temp_pv\n",
       "0         8       69.6       189       67.2      3.01       67.1\n",
       "1         8       69.8       189       67.2      3.01       67.0\n",
       "2         8       69.7       189       67.9      3.08       65.9\n",
       "3         8       69.7       189       67.8      3.08       65.9\n",
       "4         8       69.7       189       67.8      3.08       65.9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load the data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../DATA/바웰공정데이터.csv')\n",
    "\n",
    "# 2. Preprocessing : 목요일까지의 전처리\n",
    "# (1) 2 < scale_pv < 4\n",
    "data = data[(data['scale_pv'] > 2) & (data['scale_pv'] < 4)]\n",
    "\n",
    "# (2) k_rpm_pv 가 100 이하인 행 제거\n",
    "data = data[data['k_rpm_pv'] > 100]\n",
    "\n",
    "# (3) n_temp_sv=0 인 행 제거\n",
    "data = data[data['n_temp_sv'] != 0]\n",
    "\n",
    "# (4) 컬럼 제거 : E_scr_sv, c_temp_sv, n_temp_sv, s_temp_sv, k_rpm_sv, time\n",
    "data = data.drop(['E_scr_sv', 'c_temp_sv', 'n_temp_sv', 's_temp_sv', \"k_rpm_sv\", 'time'], axis=1)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-2. Preprocessing : 추가 전처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2519\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 중복값 확인\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# 중복값 제거\n",
    "data = data.drop_duplicates()\n",
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000196 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 209\n",
      "[LightGBM] [Info] Number of data points in the train set: 20462, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 3.042010\n",
      "NoScaler_LinearRegression - MAE : 0.0282\n",
      "NoScaler_LinearRegression - MAPE : 0.009285\n",
      "NoScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "NoScaler_ElasticNet - MAE : 0.0288\n",
      "NoScaler_ElasticNet - MAPE : 0.009469\n",
      "NoScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "NoScaler_RandomForestRegressor - MAE : 0.0226\n",
      "NoScaler_RandomForestRegressor - MAPE : 0.007440\n",
      "NoScaler_RandomForestRegressor - R2 : 0.4153\n",
      "----------------------------------\n",
      "NoScaler_LGBMRegressor - MAE : 0.0256\n",
      "NoScaler_LGBMRegressor - MAPE : 0.008420\n",
      "NoScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "MinMaxScaler_LinearRegression - MAE : 0.0282\n",
      "MinMaxScaler_LinearRegression - MAPE : 0.009285\n",
      "MinMaxScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "StandardScaler_LinearRegression - MAE : 0.0282\n",
      "StandardScaler_LinearRegression - MAPE : 0.009285\n",
      "StandardScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "RobustScaler_LinearRegression - MAE : 0.0282\n",
      "RobustScaler_LinearRegression - MAPE : 0.009285\n",
      "RobustScaler_LinearRegression - R2 : 0.0344\n",
      "----------------------------------\n",
      "MinMaxScaler_ElasticNet - MAE : 0.0288\n",
      "MinMaxScaler_ElasticNet - MAPE : 0.009469\n",
      "MinMaxScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "StandardScaler_ElasticNet - MAE : 0.0288\n",
      "StandardScaler_ElasticNet - MAPE : 0.009469\n",
      "StandardScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "RobustScaler_ElasticNet - MAE : 0.0288\n",
      "RobustScaler_ElasticNet - MAPE : 0.009469\n",
      "RobustScaler_ElasticNet - R2 : -0.0000\n",
      "----------------------------------\n",
      "MinMaxScaler_RandomForestRegressor - MAE : 0.0226\n",
      "MinMaxScaler_RandomForestRegressor - MAPE : 0.007438\n",
      "MinMaxScaler_RandomForestRegressor - R2 : 0.4178\n",
      "----------------------------------\n",
      "StandardScaler_RandomForestRegressor - MAE : 0.0226\n",
      "StandardScaler_RandomForestRegressor - MAPE : 0.007422\n",
      "StandardScaler_RandomForestRegressor - R2 : 0.4219\n",
      "----------------------------------\n",
      "RobustScaler_RandomForestRegressor - MAE : 0.0226\n",
      "RobustScaler_RandomForestRegressor - MAPE : 0.007443\n",
      "RobustScaler_RandomForestRegressor - R2 : 0.4187\n",
      "----------------------------------\n",
      "MinMaxScaler_LGBMRegressor - MAE : 0.0256\n",
      "MinMaxScaler_LGBMRegressor - MAPE : 0.008420\n",
      "MinMaxScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "StandardScaler_LGBMRegressor - MAE : 0.0256\n",
      "StandardScaler_LGBMRegressor - MAPE : 0.008420\n",
      "StandardScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n",
      "RobustScaler_LGBMRegressor - MAE : 0.0256\n",
      "RobustScaler_LGBMRegressor - MAPE : 0.008420\n",
      "RobustScaler_LGBMRegressor - R2 : 0.2656\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Feature Engineering\n",
    "# - Pileline으로 스케일링 및 모델링을 한번에 처리\n",
    "# - Scaling : MinMaxScaler, StandardScaler, RobustScaler, 스케일링 없이 하나\n",
    "# - Model : LinearRegression, ElasticNet, RandomForest, LightGBM\n",
    "# - Evaluation : MAE, MAPE, R2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 3-1. 데이터 분할\n",
    "X = data.drop('scale_pv', axis=1)\n",
    "y = data['scale_pv']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Create pipelines for each scaling method and model\n",
    "pipelines = {    \n",
    "    'NoScaler_LinearRegression': Pipeline([('model', LinearRegression())]),\n",
    "    'NoScaler_ElasticNet': Pipeline([('model', ElasticNet())]),\n",
    "    'NoScaler_RandomForestRegressor': Pipeline([('model', RandomForestRegressor())]),\n",
    "    'NoScaler_LGBMRegressor': Pipeline([('model', LGBMRegressor())]),\n",
    "    'MinMaxScaler_LinearRegression': Pipeline([('scaler', MinMaxScaler()), ('model', LinearRegression())]),\n",
    "    'StandardScaler_LinearRegression': Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())]),\n",
    "    'RobustScaler_LinearRegression': Pipeline([('scaler', RobustScaler()), ('model', LinearRegression())]),\n",
    "    'MinMaxScaler_ElasticNet': Pipeline([('scaler', MinMaxScaler()), ('model', ElasticNet())]),\n",
    "    'StandardScaler_ElasticNet': Pipeline([('scaler', StandardScaler()), ('model', ElasticNet())]),\n",
    "    'RobustScaler_ElasticNet': Pipeline([('scaler', RobustScaler()), ('model', ElasticNet())]),\n",
    "    'MinMaxScaler_RandomForestRegressor': Pipeline([('scaler', MinMaxScaler()), ('model', RandomForestRegressor())]),\n",
    "    'StandardScaler_RandomForestRegressor': Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())]),\n",
    "    'RobustScaler_RandomForestRegressor': Pipeline([('scaler', RobustScaler()), ('model', RandomForestRegressor())]),\n",
    "    'MinMaxScaler_LGBMRegressor': Pipeline([('scaler', MinMaxScaler()), ('model', LGBMRegressor())]),\n",
    "    'StandardScaler_LGBMRegressor': Pipeline([('scaler', StandardScaler()), ('model', LGBMRegressor())]),\n",
    "    'RobustScaler_LGBMRegressor': Pipeline([('scaler', RobustScaler()), ('model', LGBMRegressor())]),\n",
    "}\n",
    "\n",
    "# Fit the pipeline\n",
    "for pipeline in pipelines.values():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "# Evaluate the pipelines\n",
    "for name, pipeline in pipelines.items():\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(f'{name} - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "    print(f'{name} - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "    print(f'{name} - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    # MAPE가 가장 낮은 모델을 선택\n",
    "    if name == 'NoScaler_LinearRegression':\n",
    "        best_model = pipeline\n",
    "        best_scaler = 'NoScaler'\n",
    "        best_model_name = 'LinearRegression'\n",
    "        best_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    else:\n",
    "        if mean_absolute_percentage_error(y_test, y_pred) < best_mape:\n",
    "            best_model = pipeline\n",
    "            best_scaler = name.split('_')[0]\n",
    "            best_model_name = name.split('_')[1]\n",
    "            best_mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model : StandardScaler_RandomForestRegressor\n",
      "Best MAPE : 0.007422\n",
      "Best R2 : 0.4219\n"
     ]
    }
   ],
   "source": [
    "# Show the best model\n",
    "print(f'Best Model : {best_scaler}_{best_model_name}')\n",
    "print(f'Best MAPE : {best_mape:.6f}')\n",
    "print(f'Best R2 : {r2_score(y_test, best_model.predict(X_test)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "512/512 [==============================] - 3s 3ms/step - loss: 0.5810 - val_loss: 0.0027\n",
      "Epoch 2/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0027 - val_loss: 0.0023\n",
      "Epoch 3/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0023 - val_loss: 0.0025\n",
      "Epoch 4/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 5/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0021 - val_loss: 0.0018\n",
      "Epoch 6/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 7/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 8/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 9/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 10/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 11/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 12/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 13/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 14/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0018\n",
      "Epoch 15/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 16/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 17/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0020 - val_loss: 0.0026\n",
      "Epoch 18/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 19/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 20/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 21/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 22/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 23/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 24/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 25/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 26/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 27/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 28/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 29/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 30/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 31/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0027\n",
      "Epoch 32/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 33/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 34/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 35/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 36/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 37/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 38/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 39/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 40/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 41/1000\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 41: early stopping\n",
      "160/160 [==============================] - 0s 869us/step\n",
      "LSTM - MAE : 0.0292\n",
      "LSTM - MAPE : 0.009630\n",
      "LSTM - R2 : -0.0062\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델링\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# LSTM 모델링\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(X_train_scaled.shape[1], 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "\n",
    "# Reshape\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "# Evaluate\n",
    "print(f'LSTM - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'LSTM - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'LSTM - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "512/512 [==============================] - 6s 6ms/step - loss: 0.3779 - val_loss: 0.0075\n",
      "Epoch 2/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0408 - val_loss: 0.0020\n",
      "Epoch 3/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0375 - val_loss: 0.0043\n",
      "Epoch 4/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0373 - val_loss: 0.0033\n",
      "Epoch 5/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0366 - val_loss: 0.0033\n",
      "Epoch 6/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0358 - val_loss: 0.0020\n",
      "Epoch 7/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0338 - val_loss: 0.0034\n",
      "Epoch 8/1000\n",
      "512/512 [==============================] - 3s 6ms/step - loss: 0.0322 - val_loss: 0.0018\n",
      "Epoch 9/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0301 - val_loss: 0.0030\n",
      "Epoch 10/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0288 - val_loss: 0.0018\n",
      "Epoch 11/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0264 - val_loss: 0.0026\n",
      "Epoch 12/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0244 - val_loss: 0.0019\n",
      "Epoch 13/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0223 - val_loss: 0.0018\n",
      "Epoch 14/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0203 - val_loss: 0.0021\n",
      "Epoch 15/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0187 - val_loss: 0.0019\n",
      "Epoch 16/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0170 - val_loss: 0.0019\n",
      "Epoch 17/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0156 - val_loss: 0.0021\n",
      "Epoch 18/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0138 - val_loss: 0.0017\n",
      "Epoch 19/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0118 - val_loss: 0.0017\n",
      "Epoch 20/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0109 - val_loss: 0.0020\n",
      "Epoch 21/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0095 - val_loss: 0.0017\n",
      "Epoch 22/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0083 - val_loss: 0.0019\n",
      "Epoch 23/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0073 - val_loss: 0.0017\n",
      "Epoch 24/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0064 - val_loss: 0.0017\n",
      "Epoch 25/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0056 - val_loss: 0.0018\n",
      "Epoch 26/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0048 - val_loss: 0.0017\n",
      "Epoch 27/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 28/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0037 - val_loss: 0.0017\n",
      "Epoch 29/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0033 - val_loss: 0.0017\n",
      "Epoch 30/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 31/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 32/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 33/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 34/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 35/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0020 - val_loss: 0.0017\n",
      "Epoch 36/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 37/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 38/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 39/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 40/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 41/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 42/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 43/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 44/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 45/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 46/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 47/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 48/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 49/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 50/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 51/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 52/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 53/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 54/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 55/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 56/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 57/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 58/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 59/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 60/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 61/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 62/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 63/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 64/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 65/1000\n",
      "512/512 [==============================] - 3s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 66/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 67/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 68/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 69/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 70/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 71/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 72/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 73/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 74/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 75/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 76/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 77/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 78/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 79/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 80/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 81/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 82/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 83/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 84/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 85/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 86/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 87/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 88/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 89/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 90/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 91/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 92/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 93/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 94/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 95/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 96/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 97/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 98/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 99/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 100/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 101/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 102/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 103/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 104/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 105/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 106/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 107/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 108/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 109/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 110/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 111/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 112/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 113/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 114/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 115/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 116/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 117/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 118/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 119/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 120/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 121/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 122/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 123/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 124/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 125/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 126/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 127/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 128/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 129/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 130/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 131/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 132/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 133/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 134/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 135/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 136/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 137/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 138/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 139/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 140/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 141/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 142/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 143/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 144/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 145/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 146/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 147/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 148/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 149/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 150/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 151/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 152/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 153/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 154/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 155/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 156/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 157/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 158/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 159/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 160/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 161/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 162/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 163/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 164/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 165/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 166/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 167/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 168/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 169/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 170/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 171/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 172/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 173/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 174/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 175/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 176/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 177/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 178/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 179/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 180/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 181/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 182/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 183/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 184/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 185/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 186/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 187/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 188/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 189/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 190/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 191/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 192/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 193/1000\n",
      "512/512 [==============================] - 2s 5ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 193: early stopping\n",
      "160/160 [==============================] - 1s 2ms/step\n",
      "LSTM - MAE : 0.0260\n",
      "LSTM - MAPE : 0.008549\n",
      "LSTM - R2 : 0.2572\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# 데이터 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "\n",
    "# LSTM 모델링\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X_train_scaled.shape[1], 1), return_sequences=True))   # return_sequences=True : 다음 LSTM 레이어에 전달\n",
    "model.add(Dropout(0.2))         # 과적합 방지\n",
    "model.add(LSTM(64))             # return_sequences=False : Dense 레이어에 전달\n",
    "model.add(Dropout(0.2))     \n",
    "model.add(Dense(1))            # 출력 레이어\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "\n",
    "# Fit\n",
    "model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=1000, batch_size=32, callbacks=[es])\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "# Evaluate\n",
    "print(f'LSTM - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'LSTM - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'LSTM - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "540 fits failed out of a total of 1620.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "158 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "382 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\HOME\\KDT5\\KDT5_Notes\\.conda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [        nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.02321037 -0.02318184 -0.02314804\n",
      " -0.02323867 -0.02317189 -0.0231598  -0.0233736  -0.02335549 -0.0233262\n",
      " -0.02341289 -0.02339148 -0.02339357 -0.02340419 -0.02338292 -0.02336539\n",
      " -0.02354822 -0.02353781 -0.02352788 -0.02411196 -0.02410312 -0.02408227\n",
      " -0.02411808 -0.0241002  -0.02406409 -0.02411793 -0.02408705 -0.02408672\n",
      " -0.02322437 -0.0231537  -0.02315776 -0.02320221 -0.02316227 -0.02314438\n",
      " -0.02337348 -0.02332632 -0.02334039 -0.02341359 -0.02338564 -0.0233656\n",
      " -0.02341986 -0.0234015  -0.02337301 -0.02359435 -0.02355243 -0.02352629\n",
      " -0.02414277 -0.02408668 -0.02407452 -0.02412351 -0.02409236 -0.02406383\n",
      " -0.02411244 -0.02407776 -0.02406925         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.0255646  -0.02551335 -0.02553127 -0.02559765 -0.02557327 -0.02558189\n",
      " -0.02565836 -0.02565886 -0.02563291 -0.02562876 -0.02564037 -0.02563063\n",
      " -0.02565695 -0.02564607 -0.02562919 -0.02565231 -0.02568599 -0.02566505\n",
      " -0.0258015  -0.02578852 -0.02580692 -0.02583817 -0.02581067 -0.02580554\n",
      " -0.02584014 -0.02579529 -0.02581096 -0.02554682 -0.02554778 -0.02553887\n",
      " -0.02560042 -0.02557934 -0.02558498 -0.02570055 -0.02566116 -0.0256551\n",
      " -0.02565608 -0.02564634 -0.02563391 -0.02562724 -0.02563742 -0.02563089\n",
      " -0.02565926 -0.02568282 -0.02567171 -0.0258194  -0.02581969 -0.02581323\n",
      " -0.0257885  -0.02578278 -0.02580032 -0.02582732 -0.02580875 -0.02582195\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan -0.02281518 -0.02278578 -0.02278329\n",
      " -0.02303945 -0.02301741 -0.02302806 -0.02338364 -0.0233049  -0.02332227\n",
      " -0.02344335 -0.02335387 -0.02332203 -0.02337006 -0.02335225 -0.02334947\n",
      " -0.02355288 -0.02353937 -0.02352391 -0.02415491 -0.02409559 -0.0241298\n",
      " -0.02415395 -0.02414161 -0.02409183 -0.02415436 -0.02412562 -0.02410973\n",
      " -0.02279124 -0.02275889 -0.02274371 -0.02310891 -0.02300374 -0.02300998\n",
      " -0.02337206 -0.0233327  -0.02332382 -0.02337749 -0.0233712  -0.02337344\n",
      " -0.02340707 -0.02337952 -0.02336343 -0.02358662 -0.02356063 -0.0235165\n",
      " -0.02415319 -0.0241226  -0.02414447 -0.0241348  -0.02413354 -0.02411914\n",
      " -0.02414652 -0.02414096 -0.02411607         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.02320318 -0.02315266 -0.02315502 -0.02316211 -0.02314126 -0.02314008\n",
      " -0.02340775 -0.02330858 -0.02332535 -0.02345911 -0.02337235 -0.02335126\n",
      " -0.02341086 -0.02339841 -0.02336137 -0.02355216 -0.02352829 -0.02349955\n",
      " -0.02413765 -0.02407968 -0.02409003 -0.02411304 -0.02409371 -0.02408027\n",
      " -0.02411562 -0.02411706 -0.02408672 -0.02317178 -0.02314461 -0.02312269\n",
      " -0.02319642 -0.02313015 -0.02313588 -0.02338376 -0.02332314 -0.02331529\n",
      " -0.0234082  -0.02339299 -0.02337586 -0.0234307  -0.02338273 -0.02337028\n",
      " -0.02356524 -0.02353902 -0.02351752 -0.02411129 -0.02409707 -0.02407715\n",
      " -0.02411361 -0.02410362 -0.02410518 -0.02415909 -0.02407022 -0.02408547]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__max_depth': 20, 'model__max_features': 'log2', 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'model__n_estimators': 300}\n",
      "-0.02274371459495446\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 RandomForestRegressor(max_depth=20, max_features='log2',\n",
      "                                       n_estimators=300))])\n",
      "----------------------------------\n",
      "Best Model - MAE : 0.0223\n",
      "Best Model - MAPE : 0.007349\n",
      "Best Model - R2 : 0.4423\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 4. Hyperparameter Tuning : GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', RandomForestRegressor())])\n",
    "\n",
    "# Create a parameter grid : RandomForestRegressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 10, 20, 30],\n",
    "    'model__min_samples_split': [2, 4, 6],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1, verbose=3)\n",
    "\n",
    "# Fit the GridSearchCV object\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Show the best hyperparameters\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)\n",
    "print('----------------------------------')\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred = grid.predict(X_test)\n",
    "print(f'Best Model - MAE : {mean_absolute_error(y_test, y_pred):.4f}')\n",
    "print(f'Best Model - MAPE : {mean_absolute_percentage_error(y_test, y_pred):.6f}')\n",
    "print(f'Best Model - R2 : {r2_score(y_test, y_pred):.4f}')\n",
    "print('----------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EXAM_MML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
